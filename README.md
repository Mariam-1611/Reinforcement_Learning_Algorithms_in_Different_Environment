# Reinforcement Learning Algorithms in Different Environments

This project is an **interactive web dashboard** built with **Flask + Vanilla JavaScript** to visualize how classical **Reinforcement Learning (RL)** algorithms behave in different environments.

It focuses on making **value functions, policies, and agent behavior** visible and intuitive for students and practitioners.

---

## ğŸŒ Live Demo

> [`http://127.0.0.1:5000/`](http://127.0.0.1:5000)

---

## ğŸ–¼ï¸ Project Preview

<img width="1743" height="600" alt="image" src="https://github.com/user-attachments/assets/c47aa135-8d01-402b-9ae2-e73c3b5a5eff" />


<img width="1317" height="865" alt="image" src="https://github.com/user-attachments/assets/46283162-103a-4a85-9565-2485f621657c" />


---

## âœ¨ Features

### ğŸŒ Environments
- **GridWorldEnvironment**
  - Deterministic grid
  - Start, Goal, wall penalties
- **FrozenLakEnviroment**
  - Stochastic FrozenLake-style grid
  - Holes + slip probability

### ğŸ§  Algorithms
- Policy Evaluation
- Policy Improvement
- Policy Iteration
- Value Iteration
- Monte Carlo (Every-Visit & First-Visit)
- Temporal Difference **TD(0)**

### ğŸ“Š Visualizations
- Color-coded grid:
  - `S` â€“ Start
  - `G` â€“ Goal
  - `H` â€“ Hole
  - `F` â€“ Frozen / Free
- Policy arrows (â†‘ â†“ â† â†’)
- Animated agent movement
- Episode return plots (Chart.js)
- Tables for state values & rewards

### ğŸ›ï¸ Interactive Parameters
- Grid size (rows, cols)
- Slip probability (FrozenLake)
- Discount factor `Î³`
- Tolerance `Î¸`
- Episodes, learning rate `Î±`, exploration `Îµ`
- Stochastic simulation toggle
- Episode trace history

---

## ğŸ§  Algorithms

All algorithms share a **dictionary-based implementation style** for clarity:

```python
v[state]
Q[state][action]
policy[state]
```

### Dynamic Programming (Model-Based)

#### Policy Evaluation

```python
def policy_evaluation(givenPolicy, enviro, theta=0.0001, MAX=1000, gamma=0.9):
    v = {s: 0.0 for s in enviro.states}

    for _ in range(MAX):
        delta = 0.0
        for s in enviro.states:
            old_value = v[s]
            new_value = 0.0
            action = givenPolicy[s]

            for prob, next_state, reward in enviro.Prob[s][action]:
                new_value += prob * (reward + gamma * v[next_state])

            v[s] = new_value
            delta = max(delta, abs(new_value - old_value))

        if delta < theta:
            break

    return v
```

- Policy Improvement
- Policy Iteration (with history)
- Value Iteration (with optional trace)

---

### Monte Carlo (Model-Free)

Located in `Algorithms/Monte_CarloTypes.py`

- `monte_carlo_every_visit(...)`
- `monte_carlo_first_visit(...)`

Features:
- Episode generation
- Optional stochastic transitions
- Return tracking for learning curves

---

### Temporal Difference TD(0)

Located in `Algorithms/Temporal_Differance.py`

Update rule:

\[
Q(s,a) \leftarrow Q(s,a) + \alpha (r + \gamma \max_{a'} Q(s',a') - Q(s,a))
\]

Supports:
- Îµ-greedy exploration
- Episode return history

---

## ğŸŒ Environments

### 1ï¸âƒ£ GridWorldEnvironment

- States: `(i, j)`
- Actions: `up, down, left, right`
- Deterministic transitions
- Rewards:
  - Wall â†’ `-1`
  - Normal â†’ `0`
  - Goal â†’ `+10`

---

### 2ï¸âƒ£ FrozenLakEnviroment

Extends GridWorld with:
- Random holes
- Slip probability
- Stochastic transitions
- Rewards:
  - Goal â†’ `+10`
  - Hole / invalid â†’ `-1`
  - Otherwise â†’ `0`

---

## ğŸ”Œ Backend API (Flask)

### Routes

- `GET /`
- `GET /env/<env_name>`
- `GET /api/run_algorithm`
- `POST /api/simulate_policy`

Example response:

```json
{
  "policy": { "(i,j)": "action" },
  "values": { "(i,j)": 1.23 },
  "episode_returns": [],
  "trajectory": [],
  "env": "frozenlake",
  "rows": 4,
  "cols": 4
}
```

---

## ğŸ¨ Frontend

### Tech Stack
- HTML + CSS
- Vanilla JavaScript
- Chart.js

### UI Components
- Environment selection cards
- Algorithm control panel
- Grid visualization
- Agent animation
- Value & reward tables
- Learning curves

---

## âš™ï¸ Running the App

### 1. Clone the repository

```bash
git clone https://github.com/Mariam-1611/Reinforcement_Learning_Algorithms_in_Different_Environment.git
cd Reinforcement_Learning_Algorithms_in_Different_Environment
```

### 2. Create virtual environment (optional)

```bash
python -m venv .venv
# Windows
.venv\Scripts\activate
# Linux / Mac
source .venv/bin/activate
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Run the server

```bash
python app.py
```

Open:

```
http://127.0.0.1:5000
```

---

## ğŸ§ª Project Structure

```text
Reinforcement_Learning_Algorithms_in_Different_Environment/
â”‚
â”œâ”€ Algorithms/
â”‚  â”œâ”€ Policy_Iteration.py
â”‚  â”œâ”€ Value_Iteration.py
â”‚  â”œâ”€ Monte_CarloTypes.py
â”‚  â””â”€ Temporal_Differance.py
â”‚
â”œâ”€ GridWorld_Enviroment.py
â”œâ”€ FrozenLake_Enviroment.py
â”œâ”€ app.py
â”œâ”€ static/
â”‚  â”œâ”€ style.css
â”‚  â””â”€ script.js
â”œâ”€ templates/
â”‚  â”œâ”€ index.html
â”‚  â””â”€ env_detail.html
â”œâ”€ requirements.txt
â””â”€ README.md
```

## ğŸ‘©â€ğŸ’» Author

- **Name:** Mariam  
- **GitHub:** [@Mariam-1611](https://github.com/Mariam-1611)

---
